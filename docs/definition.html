<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Limitations | STA6257 Cross Validation</title>
  <meta name="description" content="Definition | STA6257 Cross Validation">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Limitations | STA6257 Cross Validation" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Definition | STA6257 Cross Validation" />
  
  
  

<meta name="author" content="Mary Tarabocchia, Jason Heiserman, Daniel Bohorquez">


<meta name="date" content="2022-10-20">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="overfitting.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Cross Validation Research Project</a></li>

<li class="divider"></li>
<li><a href="index.html#index" id="Introduction"><span class="toc-section-number"><b>1</b></span> Introduction</a></li>
<ol>
<li><a href="definition.html#definition" id="Definition"><span class="toc-section-number"><b>1.1</b></span> Definition</a></li>
<li><a href="overfitting.html#overfitting" id="Overfitting"><span class="toc-section-number"><b>1.2</b></span> Overfitting</a></li>
</ol>
<li><a href="cross-validation.html#cross-validation" id="Types of Cross-Validation"><span class="toc-section-number"><b>2</b></span> Types of Cross-Validation</a></li>
<ol>
<li><a href="k-fold.html#k-fold" id="K-Fold Cross-Validation"><span class="toc-section-number"><b>2.1</b></span> K-Fold Cross-Validation</a></li>
<li><a href="stratified.html#stratified" id="Stratified K-Fold Cross-Validation"><span class="toc-section-number"><b>2.2</b></span> Stratified K-Fold Cross-Validation</a></li>
<li><a href="holdout-method.html#holdout-method" id="Holdout Method"><span class="toc-section-number"><b>2.3</b></span> Holdout Method</a></li>
<li><a href="leave-one-out.html#leave-one-out" id="Leave-One-Out Cross-Validation"><span class="toc-section-number"><b>2.4</b></span> Leave-One-Out Cross-Validation</a></li>
<li><a href="leave-p-out.html#leave-p-out" id="Leave-P-Out Cross-Validation"><span class="toc-section-number"><b>2.5</b></span> Leave-P-Out Cross-Validation</a></li>
<li><a href="monte-carlo.html#monte-carlo" id="Monte Carlo Cross-Validation"><span class="toc-section-number"><b>2.6</b></span> Monte Carlo Cross-Validation</a></li>
</ol>
<li><a href="limitations.html#limitations" id="Limitations of Cross-Validation"><span class="toc-section-number"><b>3</b></span> Limitations of Cross-Validation</a></li>
<li><a href="methods.html#methods" id="Methods"><span class="toc-section-number"><b>4</b></span> Methods</a></li>
<ol>
<li><a href="data.html#data" id="Data"><span class="toc-section-number"><b>4.1</b></span> Data</a></li>
<li><a href="statistical-methods.html#statistical-method" id="Statistical-Method"><span class="toc-section-number"><b>4.2</b></span> Statistical-Method</a></li>
<ol>
<li><a href="kfold-results.html#analysis-results" id="Analysis-Results"><span class="toc-section-number"><b>4.2.1</b></span> K-Fold Testing & Results</a></li>
<li><a href="holdout-results.html#analysis-results" id="Analysis-Results"><span class="toc-section-number"><b>4.2.2</b></span> Holdout Testing & Results</a></li>
<li><a href="leave-one-results.html#analysis-results" id="Analysis-Results"><span class="toc-section-number"><b>4.4.3</b></span> Leave-One-Out Testing & Results</a></li>
</ol>
</ol>
<li><a href="conclusion.html#references" id="Conclusion"><span class="toc-section-number"><b>5</b></span> Conclusion</a></li>
<li><a href="references.html#references" id="References"><span class="toc-section-number"><b>6</b></span> References</a></li>
<li><a href="code.html#code" id="code"><span class="toc-section-number"><b></b></span> Appendix</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STA6257 Cross Validation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="definition" class="section level1" number="2">
<h2><span class="header-section-number">1.1 Definition</span></h2>
<p class=MsoNormal>&nbsp;</p>
  
<p>Cross-validation is defined as a method of estimating our model error using a single observed dataset by
separating data used for training from the data used for model selection and final accuracy.(Lau, 2020) Cross-
validation is a &quot;conceptually simple technique, easy to apply and requires no specific knowledge about the
details of the model.(Abu-Mostafa, 2012)The general approach is that the model is trained several times, and
for each time one part of the whole dataset is treated as learning data whereas the rest is used for validation.”
(Baron, 2021)</p>
<br>
<p>In many instances, multiple rounds of cross-validation are performed using different subsets, and their results
  are averaged to determine which model is a good predictor. (Shaffer, 1993)</p>
  
<h2><span class="header-section-number">1.2 Overfitting</span></h2>
<p class=MsoNormal>&nbsp;</p>
  
<p class=MsoNormal>Cross-validation is used to protect a model from overfitting. Overfitting occurs when the model fits more data
than required, and it tries to capture every datapoint fed to it. As a result, it starts capturing noise and
inaccurate information from the data set, which degrades the performance of the model. (Abu-Mostafa, 2012)
Noise in this context means data points which are meaningless or irrelevant and occur by random chance. Many
times, overfitting occurs because the training dataset is too small or is not representative of the data set as a
whole. In other words, the test set has different characteristics than the training set. Overfitted models try to
memorize all the data instead of learning the patterns hidden in the data. (Ying, 2019) </p>
  
<p class=MsoNormal>When a model is too complex, it overfits the data. This happens when the model finds patterns in the training
data that are just caused by random observations and that won’t recur in the future. (Lau, 2020) When a model
is too simple, it underfits the data. Underfitting occurs when the model fails to capture the true relationship
between the explanatory variables and the response variable. (Lau, 2020)</p>
  
<p class=MsoNormal> Analyzing the graphs below, the model labeled Underfit has many data points are misclassified.  
  On the other hand, the model labeled Overfit has allowed noise to affect the results.  The model in the middle 
  maintains a balance: it captures the structural constancy in the data to form the model, while resisting the noise 
  and refusing to let it bend its decision boundary.</p>
  
<p class=MsoNormal><img width=846 height=284 id="Picture 8"
src="images/image008.png"
alt="Diagram&#10;&#10;Description automatically generated" class="center">
  Badillo(2020)
  <br>
<br>
<p class=MsoNormal>Additionally, there is a tradeoff between bias and variance that one must consider. The graph below depicts this
balance between bias and variance. The horizontal axis is epoch, and the vertical axis is error, the blue line is
the training error and the red line is the validation error. If our model stops learning before the yellow triangle,
it’s underfitting. If our model stops learning after the yellow triangle, we get overfitting.</p>

<p class=MsoNormal><img width=446 height=284 id="Picture 1"
src="images/image001.png"
alt="Diagram&#10;&#10;Description automatically generated" class="center">
  <a href="https://www.kaggle.com/dansbecker/underfitting-and-overfitting"> Image from Kaggle.com</a>
    <br>
<br>
  </p>
<p class=MsoNormal>Another “cure” for overfitting is regularization. (Abu-Mostafa, 2012) When a model becomes too complicated, it
tends to take all the features into consideration, even though some of them have very limited effect on the final
output. (Ling, 2019) Regularization avoids overfitting by adding a penalty to the model’s loss function. (Ling,
2019) Another way to avoid overfitting is to prune the model by eliminating meaningless or irrelevant data. One
can implement pre-pruning by deleting conditions and rules from the model during the learning process or post-
pruning by removing conditions and rules from the model that were generated during the learning phase. (Ling,
2019) Another technique, known as early stopping, pauses training before the model starts learning the noise.
(Ling, 2019) Training continues up to a certain number of iterations or until a new iteration no longer improves
the performance of the model. After that point, the model begins to overfit the training data so we need to stop
the process before passing that point. (Abu-Mostafa, 2012)</p>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="overfitting.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dquez88/STA6257-Cross-Validation-Research-Project/edit/master/definition.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
