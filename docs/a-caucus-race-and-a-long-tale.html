<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:Wingdings;
	panose-1:5 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:"Calibri Light";
	panose-1:2 15 3 2 2 2 4 3 2 4;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
h1
	{mso-style-link:"Heading 1 Char";
	margin-top:12.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	line-height:107%;
	page-break-after:avoid;
	font-size:16.0pt;
	font-family:"Calibri Light",sans-serif;
	color:#2F5496;
	font-weight:normal;}
a:link, span.MsoHyperlink
	{color:#0563C1;
	text-decoration:underline;}
p
	{margin-right:0in;
	margin-left:0in;
	font-size:12.0pt;
	font-family:"Times New Roman",serif;}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:.5in;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:.5in;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
span.Heading1Char
	{mso-style-name:"Heading 1 Char";
	mso-style-link:"Heading 1";
	font-family:"Calibri Light",sans-serif;
	color:#2F5496;}
p.lw, li.lw, div.lw
	{mso-style-name:lw;
	margin-right:0in;
	margin-left:0in;
	font-size:12.0pt;
	font-family:"Times New Roman",serif;}
.MsoChpDefault
	{font-family:"Calibri",sans-serif;}
.MsoPapDefault
	{margin-bottom:8.0pt;
	line-height:107%;}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in .5in .5in 1.0in;}
div.WordSection1
	{page:WordSection1;}
 /* List Definitions */
 ol
	{margin-bottom:0in;}
ul
	{margin-bottom:0in;}
-->
</style>

</head>

<body lang=EN-US link="#0563C1" vlink="#954F72" style='word-wrap:break-word'>

<div class=WordSection1>

<p class=MsoNormal># Introduction</p>

<p class=MsoNormal>Cross-validation is defined as a method &quot;of estimating
our model error using a single observed dataset by separating data used for
training from the data used for model selection and final accuracy.&quot; (Lau,
2020) Cross-validation is a &quot;conceptually simple technique, easy to apply
and requires no specific knowledge about the details of the model.&quot; (Abu-Mostafa, 2012) In other words, in the process of cross-validation, the original data
is randomly divided into several subsets. The machine learning model trains on
all subsets, except one. After training, the model is tested on the remaining
subset. </p>

<p class=MsoNormal>In many instances, multiple rounds of cross-validation are
performed using different subsets, and their results are averaged to determine
which model is a good predictor. (Shaffer, 1993)</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal># Overfitting</p>

<p class=MsoNormal>Overfitting is a &quot;phenomenon where fitting the observed
facts (data) well no longer indicates that we will get a decent out-of-sample
error, and may actually lead to the opposite effect.&quot; (Abu-Mostafa, 2012)
Overfitting occurs when noise in the data &quot;has misled the data.&quot;
(Abu-Mostafa, 2012) “Noise” in this context means data points which do not
represent the true properties of the data, but only exist due to random chance.
(Lever, 2016) Overfitted models “tend to memorize all the data, including
unavoidable noise on the training set, instead of learning the discipline
hidden behind the data.” (Ying, 2019)  </p>

<p class=MsoNormal>Another cause of overfitting is “when the training set is
too small or is not representative of the data as a whole, the noises have a greater
chance of being learned, and later act as a basis of predictions.” (Ying, 2019)
Overfitted models “tend to memorize all the data, including unavoidable noise
on the training set, instead of learning the discipline hidden behind the
data.” (Ying, 2019)</p>

<p class=MsoNormal>Additionally, there is a tradeoff between bias and variance
that one must consider. “When a model is too complex, it overfits the data.
This happens because the model works too hard to find patterns in the training
data that are just caused by random chance.” (Lau, 2020) “When a model is too
simple, it underfits the data.  Underfitting occurs when the true relationship
between the explanatory variables and the response variable is simpler than it
is.” (Lau, 2020) In the graph below, the horizontal axis is epoch, and the
vertical axis is error, the blue line is the training error and the red line is
the validation error.  If we stop learning before the yellow triangle, it’s
underfitting. If we stop after the yellow triangle, we get overfitting.</p>

<p class=MsoNormal><img width=228 height=142 id="Picture 1"
src="images/image001.png"
alt="Diagram&#10;&#10;Description automatically generated"></p>

<p class=lw style='margin-top:0in;margin-right:0in;margin-bottom:8.0pt;
margin-left:0in;line-height:107%;background:white'><span style='font-size:11.0pt;
line-height:107%;font-family:"Calibri",sans-serif;color:#081819;background:
#FBFCF8'>Another “cure” for overfitting is regularization. (</span><span
style='font-size:11.0pt;line-height:107%;font-family:"Calibri",sans-serif;
color:black'>Abu-Mostafa, 2012)</span><span style='font-size:11.0pt;line-height:
107%;font-family:"Calibri",sans-serif;color:#081819;background:#FBFCF8'>  </span><span
style='font-size:11.0pt;line-height:107%;font-family:"Calibri",sans-serif;
color:#212529;background:white'>Regularization means “restricting a model to
avoid overfitting by shrinking the coefficient estimates to zero.”
(Abu-Mostafa, 2012) When a </span><span style='font-size:11.0pt;line-height:
107%;font-family:"Calibri",sans-serif;color:black'>model becomes too
complicated, it tends to take all the features into consideration, even though
some of them have very limited effect on the final output. (Ling, 2019) </span><span
style='font-size:11.0pt;line-height:107%;font-family:"Calibri",sans-serif;
color:#212529;background:white'>Regularization avoids overfitting by adding a
penalty to the model’s loss function. (Ling, 2019) Ying</span><span
style='font-size:11.0pt;line-height:107%;font-family:"Calibri",sans-serif;
color:black'> proposes pruning the model “to reduce classification complexity
by eliminating less meaningful, or irrelevant data.”  One can implement
pre-pruning by deleting conditions and rules from the model during the learning
process or post-pruning by removing conditions and rules from the model that
were generated during the learning phase. (Ying, 2019)</span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</p>

<p class=MsoNormal># Types of Cross-Validation</p>

<p class=MsoNormal>Depending on the availability and size of the data set,
various cross-validation techniques can be used:</p>

<p class=MsoListParagraphCxSpFirst style='text-indent:-.25in'><span
style='font-family:Symbol'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span>K-fold Cross-Validation</p>

<p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'><span
style='font-family:Symbol'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span>Stratified K-fold Cross-Validation</p>

<p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'><span
style='font-family:Symbol'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span>Holdout method</p>

<p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'><span
style='font-family:Symbol'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span>Leave-one-out Cross-Validation</p>

<p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'><span
style='font-family:Symbol'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span>Leave-p-out Cross-Validation</p>

<p class=MsoListParagraphCxSpLast style='text-indent:-.25in'><span
style='font-family:Symbol'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span>Monte Carlo Cross-Validation</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>## K-Fold Cross-Validation</p>

<p class=MsoNormal>In k-fold cross-validation the data is first partitioned
into k equally sized segments or folds. K iterations of training and validation
are performed such that within each iteration a different fold of the data is
used for validation while the remaining k-1 folds are used for training. (Lau,
2020) One negative of this method is that the data needs to be stratified or
rearranged in such a way as to ensure each fold is representative of the whole
dataset.  (Tang, 2008) </p>

<p class=MsoNormal><img width=382 height=188 id="Picture 3"
src="images/image002.png"> </p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>## Stratified K-Fold Cross-Validation</p>

<p class=MsoNormal>The stratified K-fold cross-validation method is yet another
method that involves the division of data sample sets in 'k' subsets or folds.
However, to “ensure that there is no biased division of data in 'k' folds, the
process of stratification is conducted to rearrange the data in such a manner
that each fold represents the whole data.” (Tang, 2008)</p>

<p class=MsoNormal><img width=422 height=172 id="Picture 11"
src="images/image003.png">&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>## Holdout Method</p>

<p class=MsoNormal><span style='color:#24292F;background:white'>The holdout
method is the simplest form of cross-validation</span>. For this method, the
test data is “held out” and not used during training. Hold-out validation
avoids the overlap between training data and test data, and therefore gives a
more accurate estimate of the performance of the algorithm. The downside is
that this procedure does not use all the available data and the results are
highly dependent on the choice for the training/test split. (Tang, 2008) </p>

<p class=MsoNormal><img width=355 height=74 id="Picture 5"
src="images/image004.png"></p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>## Leave-One-Out Cross-Validation</p>

<p class=MsoNormal>Leave-one-out cross-validation (LOOCV) is a special case of
k-fold cross-validation where k equals the number of observations in the data.
In other words, in each iteration all the data except for a single observation
are used for training and the model is tested on that single observation. This<span
style='color:#24292F;background:white'> is a “computationally expensive
procedure to perform so it is more</span> widely used on small datasets.” <a
name="_Hlk118044690">(Tang, 2008)</a></p>

<p class=MsoNormal><img width=306 height=222 id="Picture 8"
src="images/image005.png"></p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>## Leave-P-Out Cross-Validation</p>

<p class=MsoNormal><span style='color:#222222'>Leave-p-out cross-validation
(LpOCV)&nbsp;is a method in which p number of data points are taken out from
the total number of data samples represented by n. The model is trained on n-p
data points and later tested on p data points. The same process is repeated for
all possible combinations of p from the original sample. Finally, the results
of each iteration are averaged to attain the cross-validation accuracy. </span>(Tang,
2008)</p>

<p style='margin-top:0in;margin-right:0in;margin-bottom:8.0pt;margin-left:0in;
line-height:107%;background:#FEFEFE'><span style='color:black'><img width=311
height=245 id="Picture 9" src="images/image006.png"></span></p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>## Monte Carlo Cross-Validation</p>

<p class=MsoNormal><span style='color:#24292F;background:white'>Monte Carlo
cross-validation creates multiple random splits of the data into training and
testing sets. For each split, the model is fit to the training data, and
predictive accuracy is assessed using the testing data. The results are then
averaged over the splits. The disadvantage of this method is that some
observations may never be selected in the testing subsample, whereas others may
overlap, i.e., be selected more than once. (Lever 2016)</span></p>

<p class=MsoNormal><img width=324 height=173 id="Picture 10"
src="images/image007.png"></p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal># Limitations of Cross-Validation</p>

<p class=MsoNormal>Currently, cross-validation is &quot;widely accepted in data
mining and machine learning community and serves as a standard procedure for
performance estimation and model selection.&quot; (Tang, 2008) However,
cross-validation does present some challenges. One main drawback of
cross-validation is the need for excessive computational resources, especially
in methods such as k-fold cross-validation. Since the algorithm must be rerun
from scratch for k times, it requires k times more computation to evaluate.</p>

<p class=MsoNormal>Another limitation involves unseen data. In
cross-validation, the test dataset is the unseen data used to evaluate the
model's performance. This works in theory, however, there can never be a
comprehensive set of unseen data in practice, and one can never predict the
kind of data that the model might encounter in the future.</p>

<p class=MsoNormal>Take for example a model is built to predict an individual's
risk of contracting a specific disease. If the model is trained on data from a
research study involving only a particular population group (for example, men
aged 60 to 65), when it's applied to the general population, the predictive
performance might differ dramatically compared to the cross-validation
accuracy.</p>

<p class=MsoNormal>Finally, not only must the datasets be independently
controlled across different runs, but there also &quot;must not be any overlap
between the data used for learning and the data used for validation in the same
run.&quot; (Tang 2008) Typically, an algorithm can make more accurate
predictions on data it has seen during the learning phase as compared to data
it has not seen. For this reason, &quot;an overlap between the training and
validation set can lead to an over-estimation of the performance metric and is
forbidden.&quot; (Tang 2008)</p>

<p class=MsoNormal>&nbsp;</p>

<p style='margin-top:0in;margin-right:0in;margin-bottom:0in;margin-left:28.35pt;
text-indent:-28.35pt'><b><span style='font-size:14.0pt;font-family:"Calibri",sans-serif'>References</span></b></p>

<p style='margin-top:0in;margin-right:0in;margin-bottom:0in;margin-left:28.35pt;
text-indent:-28.35pt'><b><span style='font-size:11.0pt;font-family:"Calibri",sans-serif'>&nbsp;</span></b></p>

<p style='margin-top:0in;margin-right:0in;margin-bottom:0in;margin-left:28.35pt;
text-indent:-28.35pt'><span style='font-size:11.0pt;font-family:"Calibri",sans-serif'>Abu-Mostafa,
Yaser S., et al. “4. Overfitting.” <i>Learning from Data: A Short Course</i>,
AMLbook, 2012, pp. 119–153. </span></p>

<h1 style='margin-top:0in;margin-right:0in;margin-bottom:0in;margin-left:.5in;
margin-bottom:.0001pt;text-indent:-.5in;line-height:normal;background:white'><span
style='font-size:11.0pt;font-family:"Calibri",sans-serif;color:windowtext'>&nbsp;</span></h1>

<h1 style='margin-top:0in;margin-right:0in;margin-bottom:0in;margin-left:.5in;
margin-bottom:.0001pt;text-indent:-.5in;line-height:normal;background:white'><span
style='font-size:11.0pt;font-family:"Calibri",sans-serif;color:black'>Lau, S.
(2020). 15 The Bias-Variance Tradeoff. In <i>Learning Data Science [Book]</i>. </span><a
href="http://www.oreilly.com"><span style='font-size:11.0pt;font-family:"Calibri",sans-serif;
color:black'>www.oreilly.com</span></a><span style='font-size:11.0pt;
font-family:"Calibri",sans-serif;color:black'>. Retrieved October 17, 2022,
from https://www.oreilly.com/library/view/learning-data-science/9781098112998/</span></h1>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</p>

<p style='margin-top:0in;margin-right:0in;margin-bottom:0in;margin-left:28.35pt;
text-indent:-28.35pt'><span style='font-size:11.0pt;font-family:"Calibri",sans-serif'>Tang,
L., Refaeilzdeh, P. and Liu, H. (2008) <i>Cross-validation</i>. Available at:
http://leitang.net/papers/ency-cross-validation.pdf (Accessed: October 26,
2022). </span></p>

<p style='margin-top:0in;margin-right:0in;margin-bottom:0in;margin-left:28.35pt;
text-indent:-28.35pt'><span style='font-size:11.0pt;font-family:"Calibri",sans-serif'>&nbsp;</span></p>

<p style='margin-top:0in;margin-right:0in;margin-bottom:0in;margin-left:28.35pt;
text-indent:-28.35pt'><span style='font-size:11.0pt;font-family:"Calibri",sans-serif'>Schaffer,
C. (1993) “Selecting a classification method by cross-validation,” <i>Machine
Learning</i>, 13(1), pp. 135–143. Available at: </span><a
href="https://doi.org/10.1007/bf00993106"><span style='font-size:11.0pt;
font-family:"Calibri",sans-serif'>https://doi.org/10.1007/bf00993106</span></a><span
style='font-size:11.0pt;font-family:"Calibri",sans-serif'>. </span></p>

<p style='margin-top:0in;margin-right:0in;margin-bottom:0in;margin-left:28.35pt;
text-indent:-28.35pt'><span style='font-size:11.0pt;font-family:"Calibri",sans-serif'>&nbsp;</span></p>

<p style='margin-top:0in;margin-right:0in;margin-bottom:0in;margin-left:28.35pt;
text-indent:-28.35pt'><span style='font-size:11.0pt;font-family:"Calibri",sans-serif'>Ying,
X. (2019) “An overview of overfitting and its solutions,” <i>Journal of
Physics: Conference Series</i>, 1168(2), p. 022022. Available at: </span><a
href="https://doi.org/10.1088/1742-6596/1168/2/022022"><span style='font-size:
11.0pt;font-family:"Calibri",sans-serif'>https://doi.org/10.1088/1742-6596/1168/2/022022</span></a><span
style='font-size:11.0pt;font-family:"Calibri",sans-serif'>. </span></p>

<p style='margin-top:0in;margin-right:0in;margin-bottom:0in;margin-left:28.35pt;
text-indent:-28.35pt'><span style='font-size:11.0pt;font-family:"Calibri",sans-serif'>&nbsp;</span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='color:#222222;background:white'>Lever, J., Krzywinski, M. &amp; Altman,
N. Model selection and overfitting.&nbsp;<i>Nat Methods</i>&nbsp;<b>13</b>,
703–704 (2016). </span><a href="https://doi.org/10.1038/nmeth.3968"><span
style='background:white'>https://doi.org/10.1038/nmeth.3968</span></a></p>

<p style='margin-left:28.1pt;text-indent:-28.1pt'><span style='font-size:11.0pt;
font-family:"Calibri",sans-serif'>Irizarry, R. A. (2020). 29 Cross validation.
In <i>Introduction to data science: Data analysis and prediction algorithms with
R</i>. essay, CRC Press, Taylor &amp; Francis Group. </span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='color:#222222;background:white'>&nbsp;</span></p>

<p style='margin-left:28.35pt;text-indent:-28.35pt'>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

</div>

</body>

</html>
